{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4 Interpretable Models\n",
    "\n",
    "일부 모델의 경우에는 그 자체로 해석이 쉬운 것들이 있다. \n",
    "- Linear Regression\n",
    "- Logistic Regression\n",
    "- Decision Tree\n",
    "\n",
    "용어 설명\n",
    "- 선형성은 feature와 target의 값이 선형적인 관계인 것\n",
    "- 단조성은 feature와 target의 값이 한 방향으로만 일관되게 진행되는 것\n",
    "- interaction은 복수 개의 feature사이의 관계가 모델링되는 것 (ex. (방 개수, 집 크기) => 집값 예측)\n",
    "\n",
    "![](./images/ch04/01.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "---\n",
    "\n",
    "### 정의 및 구성\n",
    "- target의 값이 feature 값의 가중합으로 예측되는 것\n",
    "- weight for each feature, bias, error term following Gaussian dist\n",
    "- weight esitmation coms from LSME\n",
    "- weight estimation come with confidence interval \n",
    "  - 95% interval = 95 out of 100 try, the confidience interval include true weight\n",
    "![](./images/ch04/02.png)\n",
    "\n",
    "\n",
    "### 선형 모델이 가정하는 데이터 특징\n",
    "- Linearity \n",
    "  - prediction as a linear combination of features\n",
    "  - each term is additive (easy of seperation of the effect)\n",
    "- Normality\n",
    "  - target follows a normal distribution\n",
    "  - If not, use Generalized Linear Model (GLM)\n",
    "- Homoscedasticity (constant variable)\n",
    "  - variance of error be constant over entire feature space\n",
    "  - ex) 방크기 feature로 집값 예측시, 작은 방크기에서의 예측 오류와 큰 방 크기에서의 예측 오류가 다르면 안됨\n",
    "- Independece\n",
    "  - each instance is indep\n",
    "  - ex) 동일한 환자에서 연속 피뽑은 샘플 => not independent\n",
    "  - If not, use GEE\n",
    "- Fiexed features\n",
    "  - feature is not random variable, \n",
    "  - it is just constant, free of measurement errors\n",
    "  - but unrealistic assumption\n",
    "- Absence of multicollinearity \n",
    "  - It not, use interaction term\n",
    "\n",
    "\n",
    "### 선형 모델의 해석\n",
    "- For numeric feature\n",
    "  - feature의 값을 1 unit 변화시키면, target 값이 weight만큼 변한다. \n",
    "- Binary feature\n",
    "  - reference category로부터 해당 category로 변화시키면 weight만큼 변한다. \n",
    "  - ex) 날씨 = {Sunny, Cloudy}일 때 sunny가 reference\n",
    "- Categorical feature\n",
    "  - L-1 one-hot encoding\n",
    "  - same as binary feature\n",
    "- Intercept\n",
    "  - meaningless case : all numeric features set to zero, all categorical feature to references\n",
    "  - meaningful case : when all numeric featuees are standadised(mean of zero, deviation of one)\n",
    "- R-squared\n",
    "  - how much of the target variance is explained by the model\n",
    "  - higher R-squared, better model explains the data\n",
    "![](./images/ch04/03.png)\n",
    "\n",
    "- Feature Importance\n",
    "  - absolute value of t-statistic (estimated weight scaled with std)\n",
    "  - the more variance of estimated weight => we are not sure of correct importance => make less feature importance\n",
    "\n",
    "![](./images/ch04/04.png)\n",
    "\n",
    "### Example ( prediction of # of rented bikes )\n",
    "![](./images/ch04/05.png)\n",
    "\n",
    "- ex) 1도 올라가면 110개의 자전거가 추가로 랜트된다. \n",
    "- ex) 좋은 날씨 대비 비가 오면 랜트수가 1901개 줄어든다. \n",
    "- 모든 해석은 다른 feature가 고정되었을 때를 가정한다. \n",
    "- 하나의 feature unit만 증가시키는 것을 때로는 unrealistic inout이 되고 만다. \n",
    "  - ex) increase of # of room without increasing size of room => 현실적으로 비존재할 수도\n",
    "  \n",
    "### Visual Interpretation\n",
    "\n",
    "#### Weight Plot\n",
    "- median값, interval의 zero 포함, interval의 길이 등을 고려\n",
    "- 각 feature마다 scale이 다르다는 점이 문제이다. \n",
    "  - 정확히 비교할려면 모든 feature의 scale을 맞추어야 한다. \n",
    "\n",
    "![](./images/ch04/06.png)\n",
    "\n",
    "#### Effect Plot\n",
    "- effect = weight * feature value\n",
    "- weight은 feature scale에 따라 차이가 나지만, effect는 scale-independent하다. \n",
    "- 카테고리 feature는 하나의 boxplot으로 summarized되서 표시\n",
    "\n",
    "![](./images/ch04/07.png)\n",
    "\n",
    "\n",
    "### Explain Individual Predictions\n",
    "\n",
    "- 예를 들어서 다음의 feature를 갖는 하나의 data point를 설명해보자 \n",
    "![](./images/ch04/08.png)\n",
    "\n",
    "- red cross가 각 feature별 예측 기여도를 나타냄 \n",
    "- 대조적인 설명\n",
    "  - 평균치보다 낮은 주요 원인 - 낮은 온도, 2011년의 초반  \n",
    "![](./images/ch04/09.png)\n",
    "\n",
    "### Do Linear Models Create Good Explanations?\n",
    "\n",
    "No because\n",
    "- Contrastive한 설명을 할 수는 있지만, reference instance라는 것이 artificial, meaningless하다. \n",
    "  - 다만 모든 numerical feature가 standarized되고 모든 category feature가 effect coding된다면 reference instance는 일종의 평균 instance가 된다. \n",
    "  - 여전히 현실적이지 않을 수는 있지만, 상대적으로 more meaningful하다.\n",
    "- selective한 설명이지 않다. \n",
    "  - 모든 feature를 다 동원해서 설명하므로, \n",
    "  - 대안으로 sparse linear model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparse Linear Model\n",
    "---\n",
    "seletive한 설명을 위해 sparsity를 linear model 안으로 도입 \n",
    "\n",
    "### LASSO (least absolute shrinkage selection operator)\n",
    "\n",
    "최적화시에 큰 weight를 억제시키는 L1-norm 추가 \n",
    "![](./images/ch04/10.png)\n",
    "\n",
    "lambda가 크면 많은 feature weight이 0으로 된다.\n",
    "- CV를 통해서 적절한 lambda를 정하자\n",
    "![](./images/ch04/11.png)\n",
    "\n",
    "### Example with Lasso\n",
    "\n",
    "강한 lambda를 사용해서 2개의 feature만 살아남게 학습한 결과 \n",
    "![](./images/ch04/12.png)\n",
    "\n",
    "### Other Methods for Sparsity in Linear Models\n",
    "\n",
    "Preprocessing을 통한 방법들\n",
    "- manually selection of features by expert knowledge\n",
    "- univariate selection like high correlation coefficient\n",
    "\n",
    "Step-wise method\n",
    "- forward selection : 소수의 feature로부터 시작해서 하나씩 feature를 추가해가면서 \n",
    "- backward selection : 모든 feature를 다 쓴 것부터 시작해서 하나씩 빼가면서\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "### What is Wrong with Linear Regression for Classification?\n",
    "\n",
    "- linear model은 output probability를 출력해주지 못한다.\n",
    "- predicted class를 일종의 숫자로 취급하여 이들을 구분할 hyperplane을 학습하는 것 \n",
    "  - multiple calss로 확장이 안된다. label = 1,2,3 \n",
    "- linear model은 0보다 작은 값, 1보다 큰 값도 외삽한다. \n",
    "![](./images/ch04/13.png)\n",
    "\n",
    "### Theory\n",
    "\n",
    "* logistic regression model uses the logistic function to squeeze the output of a linear equation between 0 and 1.\n",
    "![](./images/ch04/14.png)\n",
    "\n",
    "### Interpretation\n",
    "\n",
    "Linear model for the log odds\n",
    "![](./images/ch04/15.png)\n",
    "![](./images/ch04/16.png)\n",
    "\n",
    "A change in a feature by one unit changes the odds ratio (multiplicative) by a factor of exp(beta).\n",
    "![](./images/ch04/17.png)\n",
    "![](./images/ch04/18.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GLM, GAM and more\n",
    "\n",
    "linear model의 가정들이 안맞을 때\n",
    "- error term does not follow gaussian => GLM\n",
    "- there are interaction terms => Add interation terms\n",
    "- relationship is non-linear => GAM\n",
    "![](./images/ch04/19.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-Gaussian Outcomes - GLMs\n",
    "\n",
    "#### outcome이 gaussian이 아닌 경우\n",
    "- a category ( cancer v.s. healthy)\n",
    "- a count\n",
    "- very skewed\n",
    "- multi-modal\n",
    "\n",
    "#### GLM \n",
    "- Keep the weighted sum of the features, \n",
    "- but allow non-Gaussian outcome distributions ( from exponential family)\n",
    "- and connect the expected mean of this distribution and the weighted sum through a possibly\n",
    "nonlinear function. (link function)\n",
    "\n",
    "![](./images/ch04/20.png)\n",
    "\n",
    "- ex) count of somthing\n",
    "  - use of Poission dist, log as link \n",
    "\n",
    "![](./images/ch04/21.png)\n",
    "\n",
    "- ex) logistic regression\n",
    "  - use of bernoulli dist, logistic ft as link\n",
    "  \n",
    "![](./images/ch04/22.png)\n",
    "\n",
    "#### Ex) # of cup of coffee for a day estimation\n",
    "\n",
    "input features\n",
    "- stress level (1-10)\n",
    "- how well slept night before (1-10)\n",
    "- working day or not \n",
    "\n",
    "Actual output (true observation) is not gaussian\n",
    "![](./images/ch04/23.png)\n",
    "\n",
    "But, linear model prediction negative cup of coffee\n",
    "![](./images/ch04/24.png)\n",
    "\n",
    "By GLM (use of possion dist, and log link )\n",
    "![](./images/ch04/25.png)\n",
    "\n",
    "#### Intepretation\n",
    "\n",
    "effect is multiplicative\n",
    "![](./images/ch04/26.png)\n",
    "![](./images/ch04/27.png)\n",
    "\n",
    "Increase of stress by one unit => increase by factor of 1.11\n",
    "\n",
    "number of coffeef on a work day is on 2.42 times number of coffees on a day off\n",
    "\n",
    "![](./images/ch04/28.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interactions\n",
    "\n",
    "ex) 방 크기와 방 개수\n",
    "\n",
    "Add interaction between category and numeric \n",
    "- rental bicycle 문제에서 휴일여부와 기온\n",
    "![](./images/ch04/29.png)\n",
    "\n",
    "Add interaction between category and category\n",
    "![](./images/ch04/30.png)\n",
    "\n",
    "Add interaction between two numerics => trivial\n",
    "\n",
    "#### Interpretation of interation\n",
    "\n",
    "somewhat complicated\n",
    "- Does the temperature have a negative effect given it is a working? The answer is no,\n",
    "- temparature 1도 상승시 영향 = temp(NoWork) + workingDay:temp = 125.4 - 21.8\n",
    "\n",
    "![](./images/ch04/31.png)\n",
    "\n",
    "#### Interaction visualization\n",
    "\n",
    "category + numeric 결합인 경우, 각 category별 다른 slop를 가지는 그래프\n",
    "![](./images/ch04/32.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-linear effects - GAM\n",
    "\n",
    "x,y사이가 선형 관계가 아닐 때 \n",
    "- simple transform like logarithm\n",
    "- categorization of feature\n",
    "- generalized additive model (GAM)\n",
    "\n",
    "![](./images/ch04/33.png)\n",
    "\n",
    "#### Generalized Additive Models (GAMs)\n",
    "\n",
    "GLM과 다른 점은 각 feature가 output과 nonlinear ft f로 매핑\n",
    "![](./images/ch04/34.png)\n",
    "\n",
    "어떻게 non-linear function을 배울 것인가? => Spline\n",
    "- ex) temperature와 rented bicycle간의 비선형 관계를 4개의 spline combination으로 fit\n",
    "![](./images/ch04/35.png)\n",
    "\n",
    "#### Interpretation of GAM\n",
    "\n",
    "no longer easy interpretable\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
