{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4 Interpretable Models\n",
    "\n",
    "일부 모델의 경우에는 그 자체로 해석이 쉬운 것들이 있다. \n",
    "- Linear Regression\n",
    "- Logistic Regression\n",
    "- Decision Tree\n",
    "\n",
    "용어 설명\n",
    "- 선형성은 feature와 target의 값이 선형적인 관계인 것\n",
    "- 단조성은 feature와 target의 값이 한 방향으로만 일관되게 진행되는 것\n",
    "- interaction은 복수 개의 feature사이의 관계가 모델링되는 것 (ex. (방 개수, 집 크기) => 집값 예측)\n",
    "\n",
    "![](./images/ch04/01.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "---\n",
    "\n",
    "### 정의 및 구성\n",
    "- target의 값이 feature 값의 가중합으로 예측되는 것\n",
    "- weight for each feature, bias, error term following Gaussian dist\n",
    "- weight esitmation coms from LSME\n",
    "- weight estimation come with confidence interval \n",
    "  - 95% interval = 95 out of 100 try, the confidience interval include true weight\n",
    "![](./images/ch04/02.png)\n",
    "\n",
    "\n",
    "### 선형 모델이 가정하는 데이터 특징\n",
    "- Linearity \n",
    "  - prediction as a linear combination of features\n",
    "  - each term is additive (easy of seperation of the effect)\n",
    "- Normality\n",
    "  - target follows a normal distribution\n",
    "  - If not, use Generalized Linear Model (GLM)\n",
    "- Homoscedasticity (constant variable)\n",
    "  - variance of error be constant over entire feature space\n",
    "  - ex) 방크기 feature로 집값 예측시, 작은 방크기에서의 예측 오류와 큰 방 크기에서의 예측 오류가 다르면 안됨\n",
    "- Independece\n",
    "  - each instance is indep\n",
    "  - ex) 동일한 환자에서 연속 피뽑은 샘플 => not independent\n",
    "  - If not, use GEE\n",
    "- Fiexed features\n",
    "  - feature is not random variable, \n",
    "  - it is just constant, free of measurement errors\n",
    "  - but unrealistic assumption\n",
    "- Absence of multicollinearity \n",
    "  - It not, use interaction term\n",
    "\n",
    "\n",
    "### 선형 모델의 해석\n",
    "- For numeric feature\n",
    "  - feature의 값을 1 unit 변화시키면, target 값이 weight만큼 변한다. \n",
    "- Binary feature\n",
    "  - reference category로부터 해당 category로 변화시키면 weight만큼 변한다. \n",
    "  - ex) 날씨 = {Sunny, Cloudy}일 때 sunny가 reference\n",
    "- Categorical feature\n",
    "  - L-1 one-hot encoding\n",
    "  - same as binary feature\n",
    "- Intercept\n",
    "  - meaningless case : all numeric features set to zero, all categorical feature to references\n",
    "  - meaningful case : when all numeric featuees are standadised(mean of zero, deviation of one)\n",
    "- R-squared\n",
    "  - how much of the target variance is explained by the model\n",
    "  - higher R-squared, better model explains the data\n",
    "![](./images/ch04/03.png)\n",
    "\n",
    "- Feature Importance\n",
    "  - absolute value of t-statistic (estimated weight scaled with std)\n",
    "  - the more variance of estimated weight => we are not sure of correct importance => make less feature importance\n",
    "\n",
    "![](./images/ch04/04.png)\n",
    "\n",
    "### Example ( prediction of # of rented bikes )\n",
    "![](./images/ch04/05.png)\n",
    "\n",
    "- ex) 1도 올라가면 110개의 자전거가 추가로 랜트된다. \n",
    "- ex) 좋은 날씨 대비 비가 오면 랜트수가 1901개 줄어든다. \n",
    "- 모든 해석은 다른 feature가 고정되었을 때를 가정한다. \n",
    "- 하나의 feature unit만 증가시키는 것을 때로는 unrealistic inout이 되고 만다. \n",
    "  - ex) increase of # of room without increasing size of room => 현실적으로 비존재할 수도\n",
    "  \n",
    "### Visual Interpretation\n",
    "\n",
    "#### Weight Plot\n",
    "- median값, interval의 zero 포함, interval의 길이 등을 고려\n",
    "- 각 feature마다 scale이 다르다는 점이 문제이다. \n",
    "  - 정확히 비교할려면 모든 feature의 scale을 맞추어야 한다. \n",
    "\n",
    "![](./images/ch04/06.png)\n",
    "\n",
    "#### Effect Plot\n",
    "- effect = weight * feature value\n",
    "- weight은 feature scale에 따라 차이가 나지만, effect는 scale-independent하다. \n",
    "- 카테고리 feature는 하나의 boxplot으로 summarized되서 표시\n",
    "\n",
    "![](./images/ch04/07.png)\n",
    "\n",
    "\n",
    "### Explain Individual Predictions\n",
    "\n",
    "- 예를 들어서 다음의 feature를 갖는 하나의 data point를 설명해보자 \n",
    "![](./images/ch04/08.png)\n",
    "\n",
    "- red cross가 각 feature별 예측 기여도를 나타냄 \n",
    "- 대조적인 설명\n",
    "  - 평균치보다 낮은 주요 원인 - 낮은 온도, 2011년의 초반  \n",
    "![](./images/ch04/09.png)\n",
    "\n",
    "### Do Linear Models Create Good Explanations?\n",
    "\n",
    "No because\n",
    "- Contrastive한 설명을 할 수는 있지만, reference instance라는 것이 artificial, meaningless하다. \n",
    "  - 다만 모든 numerical feature가 standarized되고 모든 category feature가 effect coding된다면 reference instance는 일종의 평균 instance가 된다. \n",
    "  - 여전히 현실적이지 않을 수는 있지만, 상대적으로 more meaningful하다.\n",
    "- selective한 설명이지 않다. \n",
    "  - 모든 feature를 다 동원해서 설명하므로, \n",
    "  - 대안으로 sparse linear model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparse Linear Model\n",
    "---\n",
    "seletive한 설명을 위해 sparsity를 linear model 안으로 도입 \n",
    "\n",
    "### LASSO (least absolute shrinkage selection operator)\n",
    "\n",
    "최적화시에 큰 weight를 억제시키는 L1-norm 추가 \n",
    "![](./images/ch04/10.png)\n",
    "\n",
    "lambda가 크면 많은 feature weight이 0으로 된다.\n",
    "- CV를 통해서 적절한 lambda를 정하자\n",
    "![](./images/ch04/11.png)\n",
    "\n",
    "### Example with Lasso\n",
    "\n",
    "강한 lambda를 사용해서 2개의 feature만 살아남게 학습한 결과 \n",
    "![](./images/ch04/12.png)\n",
    "\n",
    "### Other Methods for Sparsity in Linear Models\n",
    "\n",
    "Preprocessing을 통한 방법들\n",
    "- manually selection of features by expert knowledge\n",
    "- univariate selection like high correlation coefficient\n",
    "\n",
    "Step-wise method\n",
    "- forward selection : 소수의 feature로부터 시작해서 하나씩 feature를 추가해가면서 \n",
    "- backward selection : 모든 feature를 다 쓴 것부터 시작해서 하나씩 빼가면서\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "### What is Wrong with Linear Regression for Classification?\n",
    "\n",
    "- linear model은 output probability를 출력해주지 못한다.\n",
    "- predicted class를 일종의 숫자로 취급하여 이들을 구분할 hyperplane을 학습하는 것 \n",
    "  - multiple calss로 확장이 안된다. label = 1,2,3 \n",
    "- linear model은 0보다 작은 값, 1보다 큰 값도 외삽한다. \n",
    "![](./images/ch04/13.png)\n",
    "\n",
    "### Theory\n",
    "\n",
    "* logistic regression model uses the logistic function to squeeze the output of a linear equation between 0 and 1.\n",
    "![](./images/ch04/14.png)\n",
    "\n",
    "### Interpretation\n",
    "\n",
    "Linear model for the log odds\n",
    "![](./images/ch04/15.png)\n",
    "![](./images/ch04/16.png)\n",
    "\n",
    "when we increase one of the feature values by 1\n",
    "![](./images/ch04/17.png)\n",
    "![](./images/ch04/18.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
