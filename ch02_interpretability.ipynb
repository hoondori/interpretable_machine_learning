{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1 Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretable Macbine Learning 이란?\n",
    "---\n",
    "\n",
    "* ** refers to methods and models that make the behavior andpredictions of machine learning systems understandable to humans **\n",
    "* Below is Black Box Model, not interpretable\n",
    "\n",
    "![](./images/ch01/01.png)\n",
    "\n",
    "\n",
    "* Interpretability is \n",
    "  * the degree to which a human can understand the cause of a decision\n",
    "  * the degree to which a human can consistently predict the model’s result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google의 Interpretablilty on Image demonstration\n",
    "---\n",
    "\n",
    "[google distill](https://distill.pub/2018/building-blocks/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2 Interpretability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretable Macbine Learning 이 왜 중요한가?\n",
    "---\n",
    "\n",
    "#### knowing the ‘why’ can help you learn more about the problem, ex) why model fail\n",
    "* need for interpretability arise from an incompleteness in problem formalization\n",
    "\n",
    "#### Unexpected events make human curious, and needs an explanation of that\n",
    "* ex) AI 기반 대출 거부, AI 기반 면접 탈락 => 설명 요구\n",
    "* ex) 뜻밖의 추천 => 설명 요구 \n",
    "\n",
    "#### very-high risk 에서는 safty measure가 필요\n",
    "* ex) 최근 비행기 사고는 자동 항법 조정 장치의 오류 때문. 설명 여구\n",
    "\n",
    "#### Bias finding\n",
    "* ex) 특정 소수 민족 집단에 대한 많은 대출 거부 => 설명 필요\n",
    "\n",
    "#### socal acceptance\n",
    "* 사람은 AI를 의인화하려고 할 것이고, 이 때 사람과의 interaction처럼 설명 필요\n",
    "\n",
    "#### model debugging and auditting\n",
    "* An interpretation for an erroneous prediction helps to understand the cause of the error.\n",
    "\n",
    "#### 기타 관련 사항\n",
    "* Fairness = bias\n",
    "* Privacy\n",
    "* Robustness \n",
    "* Causality\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretable Macbine Learning 이 중요하지 않은 경우는?\n",
    "---\n",
    "\n",
    "* When the model has no significant impact\n",
    "* when the problem is well studied"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IML 관련 용어집\n",
    "\n",
    "#### Intrinsic or post-hoc\n",
    "* intrinsic interpretability\n",
    "  * interpretability is achieved by restricting the complexity of the machine learning model\n",
    "  * ex) Linear Model, Decision Tree\n",
    "* Post-hoc interpretability \n",
    "  * application of interpretation methods after model training\n",
    "  * ex) permutation feature importance\n",
    "\n",
    "### Result of interpretation method\n",
    "\n",
    "#### Feature summary statistic \n",
    "* single number per feature, ex) feature importance\n",
    "\n",
    "#### Feature summary visualization\n",
    "* ex) heatmap of feature importance\n",
    "* ex) partial dependency plot\n",
    "* ex) feature dector visualization of CNN \n",
    "\n",
    "#### Datapoint\n",
    "* ex) To explain the prediction of a data instance, the method finds a similar data point bychanging some of the features for which the predicted outcome changes in a relevant way\n",
    "* ex) the identification of prototypes of predicted classes.\n",
    "\n",
    "#### intrinsically interpretable model\n",
    "* approximate black-box model with an interpretable model\n",
    "* ex) LIME : DNN 기반 모델을 Linear한 interpretable model로 설명\n",
    "\n",
    "#### Model-specific or model-agnostic \n",
    "* Model-specific interpretation tools are limited to specific model classes.\n",
    "  * ex) interpretation of weight is limited to simple linear model\n",
    "  * ex) GRAD-CAM : limited to convolutional layer \n",
    "* Model-agnostic can be used to any ML model, and post-hoc\n",
    "\n",
    "#### Local or global\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scope of Interpretability\n",
    "---\n",
    "\n",
    "#### Algorithm Transparency\n",
    "* How does the algorithm create model?\n",
    "* ex) CNN in image may learn edge detectors...\n",
    "* ex) DNN are less transparent...\n",
    "\n",
    "#### Global, Holistic(총체적) Model Interpretability\n",
    "* You comprehend the entire model at once\n",
    "* holistic view of features\n",
    "* very difficult in practice\n",
    "\n",
    "#### Global Model Interpretability on a Modular Level\n",
    "* How do parts of the model affect predictions?\n",
    "* ex) BatchNorm이 미치는 영향 파악\n",
    "\n",
    "#### Local Interpretability for a Single Prediction (이 책의 주요 관심사)\n",
    "* Why did the model make a certain prediction for an instance?\n",
    "\n",
    "#### Local Interpretability for a Group of prediction\n",
    "* ex) 특정 subset of data에 대한 높은 오분류 원인 분석시 필요할 듯\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of Interpretability\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Properties of Explanations\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Human-friendly Explanations\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
