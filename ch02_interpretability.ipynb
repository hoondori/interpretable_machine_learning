{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1 Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretable Macbine Learning 이란?\n",
    "---\n",
    "\n",
    "* ** refers to methods and models that make the behavior andpredictions of machine learning systems understandable to humans **\n",
    "* Below is Black Box Model, not interpretable\n",
    "\n",
    "![](./images/ch01/01.png)\n",
    "\n",
    "\n",
    "* Interpretability is \n",
    "  * the degree to which a human can understand the cause of a decision\n",
    "  * the degree to which a human can consistently predict the model’s result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretablilty 참고 자료 \n",
    "---\n",
    "[Demo on MNIST](https://lrpserver.hhi.fraunhofer.de/handwriting-classification)\n",
    "\n",
    "[Demo on NLP](https://lrpserver.hhi.fraunhofer.de/text-classification)\n",
    "\n",
    "[google distill](https://distill.pub/2018/building-blocks/)\n",
    "\n",
    "http://interpretable-ml.org/cvpr2018tutorial/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2 Interpretability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretable Macbine Learning 이 왜 중요한가?\n",
    "---\n",
    "\n",
    "#### knowing the ‘why’ can help you learn more about the problem, ex) why model fail\n",
    "* need for interpretability arise from an incompleteness in problem formalization\n",
    "\n",
    "#### Unexpected events make human curious, and needs an explanation of that\n",
    "* ex) AI 기반 대출 거부, AI 기반 면접 탈락 => 설명 요구\n",
    "* ex) 뜻밖의 추천 => 설명 요구 \n",
    "\n",
    "#### very-high risk 에서는 safty measure가 필요\n",
    "* ex) 최근 비행기 사고는 자동 항법 조정 장치의 오류 때문. 설명 여구\n",
    "\n",
    "#### Bias finding\n",
    "* ex) 특정 소수 민족 집단에 대한 많은 대출 거부 => 설명 필요\n",
    "\n",
    "#### socal acceptance\n",
    "* 사람은 AI를 의인화하려고 할 것이고, 이 때 사람과의 interaction처럼 설명 필요\n",
    "\n",
    "#### model debugging and auditting\n",
    "* An interpretation for an erroneous prediction helps to understand the cause of the error.\n",
    "\n",
    "#### 기타 관련 사항\n",
    "* Fairness = bias\n",
    "* Privacy\n",
    "* Robustness \n",
    "* Causality\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretable Macbine Learning 이 중요하지 않은 경우는?\n",
    "---\n",
    "\n",
    "* When the model has no significant impact\n",
    "* when the problem is well studied"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IML 관련 용어집\n",
    "\n",
    "#### Intrinsic or post-hoc\n",
    "* intrinsic interpretability\n",
    "  * interpretability is achieved by restricting the complexity of the machine learning model\n",
    "  * ex) Linear Model, Decision Tree\n",
    "* Post-hoc interpretability \n",
    "  * application of interpretation methods after model training\n",
    "  * ex) permutation feature importance\n",
    "\n",
    "### Result of interpretation method\n",
    "\n",
    "#### Feature summary statistic \n",
    "* single number per feature, ex) feature importance\n",
    "\n",
    "#### Feature summary visualization\n",
    "* ex) heatmap of feature importance\n",
    "* ex) partial dependency plot\n",
    "* ex) feature dector visualization of CNN \n",
    "\n",
    "#### Datapoint\n",
    "* ex) To explain the prediction of a data instance, the method finds a similar data point bychanging some of the features for which the predicted outcome changes in a relevant way\n",
    "* ex) the identification of prototypes of predicted classes.\n",
    "\n",
    "#### intrinsically interpretable model\n",
    "* approximate black-box model with an interpretable model\n",
    "* ex) LIME : DNN 기반 모델을 Linear한 interpretable model로 설명\n",
    "\n",
    "#### Model-specific or model-agnostic \n",
    "* Model-specific interpretation tools are limited to specific model classes.\n",
    "  * ex) interpretation of weight is limited to simple linear model\n",
    "  * ex) GRAD-CAM : limited to convolutional layer \n",
    "* Model-agnostic can be used to any ML model, and post-hoc\n",
    "\n",
    "#### Local or global\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scope of Interpretability\n",
    "---\n",
    "\n",
    "#### Algorithm Transparency\n",
    "* How does the algorithm create model?\n",
    "* ex) CNN in image may learn edge detectors...\n",
    "* ex) DNN are less transparent...\n",
    "\n",
    "#### Global, Holistic(총체적) Model Interpretability\n",
    "* You comprehend the entire model at once\n",
    "* holistic view of features\n",
    "* very difficult in practice\n",
    "\n",
    "#### Global Model Interpretability on a Modular Level\n",
    "* How do parts of the model affect predictions?\n",
    "* ex) BatchNorm이 미치는 영향 파악\n",
    "\n",
    "#### Local Interpretability for a Single Prediction (이 책의 주요 관심사)\n",
    "* Why did the model make a certain prediction for an instance?\n",
    "\n",
    "#### Local Interpretability for a Group of prediction\n",
    "* ex) 특정 subset of data에 대한 높은 오분류 원인 분석시 필요할 듯\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of Interpretability\n",
    "---\n",
    "\n",
    "* Application level evaluation (real task)\n",
    "  * Make it product, and be evaluated by End User (Domain Expert)\n",
    "  * ex) 의료 쪽 이상 탐지 시스템의 설명을 전문가인 의사가 듣고 평가 \n",
    "* Human level evaluation (simple task)\n",
    "  * 쉬운 설명을 비전문가가 평가 \n",
    "* Function level evaluation (proxy task)\n",
    "  * 사실상 자동화된 평가\n",
    "  * ex) Shorter Decision Tree => better explanation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Properties of Explanations\n",
    "---\n",
    "Robnik-Sikonja and Bohanec, 2018\n",
    "\n",
    "### Properties of Explanation Methods\n",
    "\n",
    "#### Expressive Power\n",
    "* 설명은 일종의 언어이다. \n",
    "* IF-Then rule, decision tree, weighted sum, NLP 등등으로 (이해 가능할 만한) 표현력이 있어야 한다.\n",
    "\n",
    "#### Translucency (투명성)\n",
    "* how much the explanation method relies on looking into ML model\n",
    "* ex) linear model은 완전 투명하다. \n",
    "* ex) Black box model은 입력 대비 출력 변화 양상 기반 설명이므로, 완전 불투명하다. \n",
    "\n",
    "#### Portability (이식성)\n",
    "* range of ML model with which explanation method can be used\n",
    "* 불투명한 설명 방법은 오히려 높은 이식성을 가진다. \n",
    "* ex) GradCAM은 CNN에서는 잘 되는데 RNN에는 적용 불가라서 이식성이 빵이다. \n",
    "\n",
    "#### Algorithm Complexity \n",
    "* 설명 방법의 계산 복잡도. \n",
    "* inference는 빠르게 되는데, 설명 생성이 너무 느리면 곤란\n",
    "\n",
    "### Properties of Indivisual Explanations\n",
    "\n",
    "#### Fidelity\n",
    "* How well does the explanation approximate the prediction of the black box model\n",
    "* ex) 기존 빚이 많고 무직이이라서 대출을 승인했습니다. => zero fidelity\n",
    "\n",
    "#### Consistency\n",
    "* 같은 데이터셋, 같은 테스크라면 모델이 달라도 같은 prediction에 대해서 비슷한 설명을 해야 일관된 설명이다. \n",
    "* 반면 모델이 너무 상이하면(ex. SVM, Linear Model), 서로 다른 feature를 쓰므로 설명도 다를 것이 당연하다. => Rashomon Effect\n",
    "* 그러나 비슷한 모델(ex. vgg16, vgg8)이라면 설명은 유사해야..\n",
    "\n",
    "#### Stability\n",
    "* How similar ar the explanations for similiar instances?\n",
    "\n",
    "#### Comprehensibility\n",
    "* How wee do human understand explanations?\n",
    "* 코끼리 다리 만지기 식으로 정확히 정의하기는 쉽지는 않지만... 가장 중요한..\n",
    "\n",
    "#### Certainty\n",
    "* reflection of model uncertainty\n",
    "* 모델의 판단 자체가 불확실하다면, 어떤 식으로 이것이 설명에도 반영되어야 한다. \n",
    "\n",
    "#### Degree of Importance\n",
    "* 설명의 각 부분부분에도 경중이 있으므로.. 이것이 잘 분간되게.. 강약중간약 설명..\n",
    "\n",
    "#### Novelty\n",
    "* 설명 대상 데이터 instance가 novelty하다면..\n",
    "* model 결과도 uncertain하고,.. 설명도 사실성 무용지물.. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Human-friendly Explanations\n",
    "---\n",
    "Miller 2017\n",
    "\n",
    "### Contrastive Explanation (Lipton 1990)\n",
    "* conterfactual explanation\n",
    "* How would the prediction have been if input X had been different?\n",
    "* ex) 이러저러해서 대출 거절했어요 => 대출을 받으신 다른 분 사례에 견주어 봐서, 이런저런 점이 보강되면 대출 승인 될 수 있어요\n",
    "* ex) 임상실험에서 실험군, 대조군\n",
    "* 머신러닝에서는 reference instance을 정하고, 이에 견주어 설명해야 한다. \n",
    "\n",
    "### Explanation are selected\n",
    "* 긴 설명(모든 원인을 다 나열하는 것은)은 거부감이 든다. \n",
    "* 핵심 원인 몇몇을 찍어서 설명\n",
    "\n",
    "### Explanation are social\n",
    "* 설명은 사람에게 하는 소통이다.\n",
    "* 머신러닝에서는 설명의 대상(explainee)이 누구인지에 따라 설명이 달라져야 한다. \n",
    "  * ex) 자율 주행차의 사고 유발 설명 => 차 소유자, 교통 당국, 법정\n",
    "\n",
    "### Explanation focus on the abnormal\n",
    "* 사람은 독특한 원인 기반 설명을 선호한다. \n",
    "* ex) 저 집은 커서 비싸 => 평범한 설명\n",
    "* ex) 저 집은 마이클 조단이 살던 대라서 비싸 => 독특한 원인 기반 설명\n",
    "\n",
    "### Explanation are truthful \n",
    "* 실제 현실에 부합해야 한다. \n",
    "* selective한 설명을 하다보면 단순화를 시도하다보니 truthful한 설명이 되지 않을수도 있다. \n",
    "\n",
    "### Consisten with prior belief of explainee\n",
    "* 사람의 직관과 선입견에 너무 벗어나는 설명이면 곤란\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
